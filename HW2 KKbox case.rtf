{\rtf1\ansi\ansicpg1252\cocoartf2511
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\paperw11900\paperh16840\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f0\fs24 \cf0 library(dplyr)\
library(ggplot2)\
library(class)\
library(rpart)\
library(ROCR)\
\
kkb<-kkbox_1_\
glimpse(kkb)\
summary(kkb)\
str(kkb)\
\
#prepare and split the data\
## one to many & filter the binary column\
x <- model.matrix(is_churn~.,data=kkb)\
\
#x <- model.matrix(Default~.,data=credit)\
\
\
#split the data\
set.seed(666)\
train<-sample(1:9000,7000)\
xtrain<- x[train,][,-1]\
xnew<- x[-train,][,-1]\
ytrain<- kkb$is_churn[train]\
ynew<-kkb$is_churn[-train]\
traindata <- data.frame(is_churn=ytrain,xtrain)\
\
\
#M1 Logistic Regression\
m1 <- glm(is_churn ~. , family = binomial,data= traindata)\
pred1<- predict.glm(m1,newdata = data.frame(xnew), type = "response")\
summary(m1)\
\
\
linearMod <- lm(is_churn ~. , traindata)\
summary(linearMod)\
\
\
#M2 Classification Tree\
m2<- rpart(formula=as.factor(is_churn)~., data = traindata, method= "class")\
summary(m2)\
\
#draw the tree\
library(rpart.plot) \
prp(m2,         \
    faclen=0,           \
    fallen.leaves=FALSE, \
    extra=2)}